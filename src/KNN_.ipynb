{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree project: diabetes prediction\n",
    "\n",
    "## Notebook set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation\n",
    "\n",
    "### 1.1. Load data from URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EDA\n",
    "### 2.1. Baseline model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here... Note: a logistic regression model gets ~77% training accuracy on the 'raw' data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Feature distributions and cleaning\n",
    "\n",
    "### 2.2.1. Distribution plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here... Watch out for missing data hiding in plain sight, 'nan' is not the only way missing values show up!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2. Missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Feature interactions & selection\n",
    "\n",
    "#### 2.3.1. Feature cross-correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2. Feature-label interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here... Note: it is pretty easy to get 100% accuracy on the training set with a decision tree model! \n",
    "# Therefore, we need a better way to measure generalizability while we are optimizing than using the test set \n",
    "# over and over (see below...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_scores={\n",
    "    'Model': [],\n",
    "    'Score': []\n",
    "}\n",
    "\n",
    "scores=cross_val_score(\n",
    "    DecisionTreeClassifier(random_state=315),\n",
    "    training_df.drop('Outcome', axis=1),\n",
    "    training_df['Outcome'],\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "cross_val_scores['Model'].extend(['Baseline']*len(scores))\n",
    "cross_val_scores['Score'].extend(scores)\n",
    "\n",
    "print(f'Cross-validation accuracy: {np.mean(scores)*100:.1f} +/- {np.std(scores)*100:.1f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter optimization\n",
    "\n",
    "### 4.1. Hyperparameter grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Hyperparameter optimization results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=pd.DataFrame(search_results.cv_results_)\n",
    "sorted_results=results.sort_values('rank_test_score')\n",
    "\n",
    "plt.title('Hyperparameter optimization')\n",
    "plt.xlabel('Hyperparameter set validation accuracy rank')\n",
    "plt.ylabel('Validation accuracy (%)')\n",
    "plt.gca().invert_xaxis()\n",
    "\n",
    "plt.fill_between(\n",
    "    sorted_results['rank_test_score'],\n",
    "    sorted_results['mean_test_score']*100 + sorted_results['std_test_score']*100,\n",
    "    sorted_results['mean_test_score']*100 - sorted_results['std_test_score']*100,\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    sorted_results['rank_test_score'],\n",
    "    sorted_results['mean_test_score']*100,\n",
    "    label='Validation'\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    sorted_results['rank_test_score'],\n",
    "    sorted_results['mean_train_score']*100 + sorted_results['std_train_score']*100,\n",
    "    sorted_results['mean_train_score']*100 - sorted_results['std_train_score']*100,\n",
    "    alpha=0.5\n",
    ")\n",
    "\n",
    "plt.plot(\n",
    "    sorted_results['rank_test_score'],\n",
    "    sorted_results['mean_train_score']*100,\n",
    "    label='Training'\n",
    ")\n",
    "\n",
    "plt.legend(loc='best', fontsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Cross-validation of optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "### 5.1. Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Test set performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n",
    "\n",
    "# Plot the confusion matrix\n",
    "cm=confusion_matrix(testing_df['Outcome'], testing_predictions, normalize='true')\n",
    "cm_disp=ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "_=cm_disp.plot()\n",
    "\n",
    "plt.title(f'Optimized model test set performance')\n",
    "plt.xlabel('Predicted outcome')\n",
    "plt.ylabel('True outcome')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
